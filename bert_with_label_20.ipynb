{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前用了一个小数据，保存为data_syn_with_label_20.csv，里面有“text”评价信息和“label”标签信息，“label”仅分为“正面”和“负面”2类。现在需要做以下操作：1.bert进行词向量表征 ，label 需要怎么处理 ； 2.创建数据集； 3.将数据集加载到简单的cnn模型中进行训练，测试是否可用；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data:                                                 text label\n",
      "0  五点 十多 去 店里 都 没开 灯 很 暗 真的 很 影响 用餐 体验 南昌 吃 感觉 完全...    负面\n",
      "1  朋友 带来 吃 说 霸占 南昌 平价 火锅 悠闲 味道 确实 不赖 四个 女生 加 三次 菜...    正面\n",
      "2                            美味 服务 好 经常 带 小孩 一家人 去 吃    正面\n",
      "3  季季红 火锅 食材 新鲜 味道 正宗 锅底 浓郁 调味 恰到好处 服务周到 热情 火锅 爱好...    正面\n",
      "4  非常 美味 真的 推荐 吃 都 美味 服务 好 态度 已经 不是 一次 两次 吃 超 喜欢 ...    正面\n"
     ]
    }
   ],
   "source": [
    "# 1. 加载数据\n",
    "# 首先，加载CSV文件，该文件包含文本评价信息和相应的标签。\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv(\"data_syn_with_label_20.csv\")\n",
    "print(\"Loaded data:\", data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding:   label  label_encoded\n",
      "0    负面              1\n",
      "1    正面              0\n"
     ]
    }
   ],
   "source": [
    "# 2. 处理标签\n",
    "# 对于二分类任务，使用 LabelEncoder 将文本标签转换为整数是合适的。这个过程将 \"正面\" 和 \"负面\" 分别映射为 0 和 1。\n",
    "# 初始化并应用LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "data['label_encoded'] = label_encoder.fit_transform(data['label'])\n",
    "\n",
    "# 显示转换后的标签以确保正确\n",
    "print(\"Label Encoding:\", data[['label', 'label_encoded']].drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv(\"data_syn_with_label_20.csv\")\n",
    "\n",
    "# 初始化BERT tokenizer和模型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize 文本\n",
    "inputs = tokenizer(data['text'].tolist(), max_length=128, truncation=True, padding='max_length', return_tensors=\"tf\")\n",
    "\n",
    "# 使用BERT模型生成词向量\n",
    "outputs = model_bert(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "bert_features = outputs.last_hidden_state\n",
    "\n",
    "# 使用平均池化获取每个句子的向量表示\n",
    "pooled_bert_features = tf.reduce_mean(bert_features, axis=1)\n",
    "\n",
    "# 处理标签\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(data['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建TensorFlow数据集\n",
    "def create_dataset(features, labels):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    dataset = dataset.shuffle(len(labels)).batch(10)  # 适当调整batch size\n",
    "    return dataset\n",
    "\n",
    "# 转换features为适合CNN输入的形式\n",
    "dataset = create_dataset(pooled_bert_features, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4000 - loss: 0.7830  \n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9333 - loss: 0.2874\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9000 - loss: 0.3357\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9333 - loss: 0.2003\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9333 - loss: 0.1898\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9333 - loss: 0.2516\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.1462\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9333 - loss: 0.2093\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9000 - loss: 0.1895\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9667 - loss: 0.1611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2252b0d92b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载至简单CNN模型进行测试是否可行\n",
    "# 调整模型输入层以适应数据形状：如果您打算使用平均池化后的BERT输出（形状为 [None, 768]），\n",
    "# 应该修改模型结构以接受这种形状的输入。由于数据已经是池化后的输出，您可能不需要一个复杂的CNN结构来处理它。一个简单的全连接层可能就足够了。\n",
    "# 修正模型结构：基于BERT的输出形状，下面是一个调整后的模型示例。\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# 创建适应平均池化输出的模型\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(768,)),  # 输入层现在直接接受平均池化的768维向量\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # 用于二分类\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit(dataset, epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这一步，说明20行的小数据实验成功运行，接下来，处理整个数据，带标签的约8千万数据 \n",
    "先进行BERT词向量表征 \n",
    "然后创建数据集\n",
    "再次生成训练集和测试集"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
