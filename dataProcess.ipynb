{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统一文本编码格式\n",
    "确保所有文本数据都采用统一的编码格式（推荐UTF-8），这有助于避免在处理文本时出现编码错误。可以在读取数据时指定编码格式，确保所有文本都是UTF-8编码，核心代码如下。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('jijihong.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        五点一十多去的店里，都没开什么灯，很暗，真的很影响用餐体验，跟我在南昌吃的感觉完全不同。我用...\n",
      "1        朋友带来吃的，说是霸占了南昌的平价火锅[悠闲] 味道确实不赖，四个女生加了三次菜，其中是三份...\n",
      "2                                        好吃，服务好经常带小孩一家人去吃，\n",
      "3        来南昌几天，经常在大街上看到季季红的广告牌，感觉应该是本地比较有名的火锅??，就来尝尝啦 [...\n",
      "4          季季红火锅，食材新鲜，味道正宗，锅底浓郁，调味恰到好处，服务周到热情，是火锅爱好者的不错选择。\n",
      "                               ...                        \n",
      "10028    第一次去吃结果就踩雷了  猪脑花是冰冻的  牛肉丸脆皮肠也是冰冻的好歹你也冲一下水在给我上呀...\n",
      "10029    上菜很慢很慢。吃到最后还有菜没上齐 而且分量比其他门店小 真的很无语吃过这么久上菜最慢最慢的...\n",
      "10030    说句实话 现在服务越来越差 点了个虾滑 我不会下 以前吃喊服务员都会帮忙下 今天我过去吃 吃...\n",
      "10031                   口味一般 环境一般 服务态度很差  在南昌吃季季红体验感最差的一次了\n",
      "10032    跨年去他们家吃火锅楼下只有方桌了，我们就两个人表示不想坐方桌，第一个服务员告知我们说楼上没有...\n",
      "Name: text, Length: 10033, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['text'])\n",
    "df['text'] = df['text'].str.replace(r'<[^>]+>', '', regex=True)  # 去除HTML标签\n",
    "df['text'] = df['text'].str.replace(r'http\\S+', '', regex=True)  # 去除URLs\n",
    "df['text'] = df['text'].str.replace(r'\\d+', '', regex=True)  # 去除数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.replace(r'\\b\\d{11}\\b', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Administrator\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.368 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.add_word(\"季季红\",freq=100)  \n",
    "df['text'] = df['text'].apply(lambda x: ' '.join(jieba.cut(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].to_csv(\"data_jieba.csv\",index=False,header=True,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        五点 十多 去 店里 都 没开 灯 很 暗 真的 很 影响 用餐 体验 南昌 吃 感觉 完全...\n",
       "1        朋友 带来 吃 说 霸占 南昌 平价 火锅 悠闲 味道 确实 不赖 四个 女生 加 三次 菜...\n",
       "2                                  好吃 服务 好 经常 带 小孩 一家人 去 吃\n",
       "3        南昌 几天 经常 大街 上 看到 季季红 广告牌 感觉 应该 本地 比较 有名 火锅 尝尝 ...\n",
       "4        季季红 火锅 食材 新鲜 味道 正宗 锅底 浓郁 调味 恰到好处 服务周到 热情 火锅 爱好...\n",
       "                               ...                        \n",
       "10028    第一次 去 吃 踩 雷 猪脑 花是 冰冻 牛肉丸 脆皮 肠 冰冻 好歹 一下 水在 上 冰箱...\n",
       "10029    上菜 很慢 很慢 吃 最后 菜 没 上 齐 分量 门店 小 真的 很无语 吃 这么久 上菜 ...\n",
       "10030    说句实话 现在 服务 越来越 差 点 虾 滑 不会 下 以前 吃 喊 服务员 都 会 帮忙 ...\n",
       "10031                    口味 环境 服务态度 很差 南昌 吃 季季红 体验 感 最差 一次\n",
       "10032    跨年 去 家 吃火锅 楼下 方桌 两个 人 表示 不想 坐 方桌 第一个 服务员 告知 说 ...\n",
       "Name: text, Length: 10033, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = set(open('stopwords_hit.txt', 'r', encoding='utf-8').read().split())\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join(word for word in x.split() if word not in stopwords))\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        五点 十多 去 店里 都 没开 灯 很 暗 真的 很 影响 用餐 体验 南昌 吃 感觉 完全...\n",
       "1        朋友 带来 吃 说 霸占 南昌 平价 火锅 悠闲 味道 确实 不赖 四个 女生 加 三次 菜...\n",
       "2                                  美味 服务 好 经常 带 小孩 一家人 去 吃\n",
       "3        南昌 几天 经常 大街 上 看到 季季红 广告牌 感觉 应该 本地 比较 有名 火锅 尝尝 ...\n",
       "4        季季红 火锅 食材 新鲜 味道 正宗 锅底 浓郁 调味 恰到好处 服务周到 热情 火锅 爱好...\n",
       "                               ...                        \n",
       "10028    第一次 去 吃 踩 雷 猪脑 花是 冰冻 牛肉丸 脆皮 肠 冰冻 好歹 一下 水在 上 冰箱...\n",
       "10029    上菜 很慢 很慢 吃 最后 菜 没 上 齐 分量 门店 小 真的 很无语 吃 这么久 上菜 ...\n",
       "10030    说句实话 现在 服务 越来越 差 点 虾 滑 不会 下 以前 吃 喊 服务员 都 会 帮忙 ...\n",
       "10031                    口味 环境 服务态度 很差 南昌 吃 季季红 体验 感 最差 一次\n",
       "10032    跨年 去 家 吃火锅 楼下 方桌 两个 人 表示 不想 坐 方桌 第一个 服务员 告知 说 ...\n",
       "Name: text, Length: 10033, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建同义词典\n",
    "synonym_dict = {\n",
    "    '好吃': '美味',\n",
    "    '赞': '美味',\n",
    "    '绝味': '美味',\n",
    "    '满意': '喜欢',\n",
    "    '合口': '美味',  # 假设'合口'是方言表达，可以替换为'美味'\n",
    "    # ... 其他同义词映射\n",
    "}\n",
    "\n",
    "# 应用同义词典，将文本中的同义词替换为标准表达\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join(synonym_dict.get(word, word) for word in x.split()))\n",
    "\n",
    "# 处理完成后的df['text']将包含统一的标准表达\n",
    "df['text'].to_csv('data_syn.csv',index=False,header=True,encoding='utf-8-sig')\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP\n",
    "df = pd.read_csv(\"data_syn.csv\",encoding='utf')\n",
    "# 先将所有非字符串类型的文本转换为字符串，然后去除或替换NaN值\n",
    "df['text'] = df['text'].astype(str)\n",
    "# 去除包含NaN文本的行\n",
    "df.dropna(subset=['text'], inplace=True)\n",
    "# SnowNLP的情感分析返回一个介于0到1之间的分数，大于0.5通常表示正面情绪\n",
    "df['sentiment_score'] = df['text'].apply(lambda x: SnowNLP(x).sentiments)\n",
    "# 将情感分数转换为情感标签\n",
    "df['sentiment_label'] = df['sentiment_score'].apply(lambda x: '正面' if x > 0.5 else '负面')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_label\n",
      "正面    8503\n",
      "负面    1530\n",
      "Name: count, dtype: int64\n",
      "正面情感占比：84.75%\n",
      "负面情感占比：15.25%\n"
     ]
    }
   ],
   "source": [
    "df['sentiment_label']\n",
    "# 统计正面和负面情感的数量\n",
    "sentiment_counts = df['sentiment_label'].value_counts()\n",
    "\n",
    "# 打印情感标签的计数结果\n",
    "print(sentiment_counts)\n",
    "\n",
    "# 计算正负情感的百分比\n",
    "total_reviews = len(df)\n",
    "positive_percent = (sentiment_counts.get('正面', 0) / total_reviews) * 100\n",
    "negative_percent = (sentiment_counts.get('负面', 0) / total_reviews) * 100\n",
    "\n",
    "# 打印情感百分比\n",
    "print(f\"正面情感占比：{positive_percent:.2f}%\")\n",
    "print(f\"负面情感占比：{negative_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将 df['sentiment_score'] 和 df['sentiment_label'] 这两列数据保存到csv文件\n",
    "df[['sentiment_score','sentiment_label']].to_csv('sentiment_data.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tag'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tag'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#在原有数据中分出正面、反面标签 \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtag\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m正面\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      4\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m正面\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtag\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m反面\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tag'"
     ]
    }
   ],
   "source": [
    "#在原有数据中分出正面、反面标签 \n",
    "import pandas as pd\n",
    "if df['tags'] .str.contains('正面'):\n",
    "    df['customer_label'] = '正面'\n",
    "elif df['tags'] .str.contains('反面'):\n",
    "    df['customer_label'] = '正面'\n",
    "# 保存标签对比文件\n",
    "df[['sentiment_label','customer_label']].to_csv('label.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
